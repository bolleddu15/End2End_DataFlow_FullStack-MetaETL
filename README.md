# End-to-End-ETL-pipeline-using-Meta-dataset

A simple yet powerful ETL pipeline built using Apache Spark, Python, SQL, and MongoDB. This project demonstrates the complete process of data extraction, transformation, and loading using a meta dataset, along with real-world case studies.

## Key Technologies

- **Apache Spark:** Distributed processing for handling big data.
- **Python:** Scripting and data transformation.
- **SQL:** Querying and manipulating data.
- **MongoDB:** NoSQL database for storing the processed data.

## Project Overview

The project covers:
- **Data Extraction:** Reading and extracting data using Apache Spark.
- **Data Transformation:** Cleaning and processing data with Python and SQL.
- **Data Loading:** Storing the final processed data into MongoDB.
- **Case Studies:** Examples to illustrate the practical use of the pipeline.

## Detailed Sections

### Heading 1

This section provides background information and key details about the project. Use it to explain the importance or context of your work.

### Heading 2

Expand on technical aspects or further details in this section. You might include configuration settings, dependencies, or any additional context needed to understand the project.

### Heading 3

Outline the step-by-step process of the ETL pipeline:
1. **Extract:** Use Apache Spark to pull raw data.
2. **Transform:** Process and clean data using Python and SQL.
3. **Load:** Insert the transformed data into MongoDB.
